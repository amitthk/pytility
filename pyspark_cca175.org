* Basics
- spark-shell
- pyspark 
  - --master spark://spark-master:7077
- SparkContext
- SQLContext
- HiveContext
- spark-sql (only latest version)
- JDBC
- To connect to remote database using jdbc
- It works only from spark 1.3.0 or later

** Official required skills

https://www.cloudera.com/about/training/certification/cca-spark.html

*** Transform, Stage, and Store
- Convert a set of data values in a given format stored in HDFS into new data values or a new data format and write them into HDFS.
- Load data from HDFS for use in Spark applications

- Write the results back into HDFS using Spark

- Read and write files in a variety of file formats

- Perform standard extract, transform, load (ETL) processes on data using the Spark API
*** Data Analysis
- Use Spark SQL to interact with the metastore programmatically in your applications. Generate reports by using queries against loaded data.
- Use metastore tables as an input source or an output sink for Spark applications

- Understand the fundamentals of querying datasets in Spark

- Filter data using Spark

- Write queries that calculate aggregate statistics

- Join disparate datasets using Spark

- Produce ranked or sorted data
*** Configuration
- This is a practical exam and the candidate should be familiar with all aspects of generating a result, not just writing code.

- Supply command-line options to change your application configuration, such as increasing available memory


** Importants
** read write files from different formats
- hive joins in spark
- connecting to mariadb and uploading data
- transformations and aggregations
** hdfs cluster cli operations
- backups and hdfs snapshots
- logs and cluster settings
- resource manager pool templates
- queues and allocations
**  spark python
- spark scala
- sqoop import
- avro
- hive queries
** import mysql to hdfs using sqoop
- export mysql to hdfs using sqoop
- change delimiter and file format of data with sqoop
- realtime / near real time with flume
- hdfs commands
** ETL
- read & write to hdfs with spark
- join, aggregate, filter using spark
- ranked and sorted results with spark
** Data Analysis
- read/create tables in hive meta store
- extract avro schema using avro tools
- store table in hive & hms in avro
- table in hive using external schema file
- improve query performance by partitioning hive table
- evolve avro schema using json files

* Either you need to run pyspark with driver-class-path or set environment variable with os.environ
#+BEGIN_SRC
spark/bin/pyspark --master spark://spark-master:7077 --driver-class-path /usr/share/java/mysql-connector-java.jar
os.environ['SPARK_CLASSPATH'] = "/usr/share/java/mysql-connector-java.jar"

from pyspark.sql import SQLContext

sqlContext = SQLContext(sc)
jdbcurl = "jdbc:mysql://quickstart.cloudera:3306/retail_db?user=retail_dba&password=cloudera"
df = sqlContext.load(source="jdbc", url=jdbcurl, dbtable="departments")

for rec in df.collect():
  print(rec)

df.count()
#+END_SRC
* <<<<<==============>>>>>

* Developing simple scala based applications for spark
* Save this to a file with py extension
#+BEGIN_SRC

from pyspark import SparkContext, SparkConf
conf = SparkConf().setAppName("pyspark")
sc = SparkContext(conf=conf)
dataRDD = sc.textFile("/data/retail_db/t0/departments")
for line in dataRDD.collect():
    print(line)
dataRDD.saveAsTextFile("/data/retail_db/pyspark/departmentsTesting")
#+END_SRC

* Run using this command
* master local will run in spark native mode
spark-submit --master local saveFile.py

* master yarn will run in yarn mode
spark-submit --master yarn saveFile.py

* <<<<<==============>>>>>

*  Load data from HDFS and storing results back to HDFS using Spark
#+BEGIN_SRC

from pyspark import SparkContext

dataRDD = sc.textFile("/data/retail_db/t0/departments")
for line in dataRDD.collect():
    print(line)

print(dataRDD.count())

dataRDD.saveAsTextFile("/data/retail_db/pyspark/departments")
#+END_SRC
* Object files are not available in python
#+BEGIN_SRC

dataRDD.saveAsObjectFile("/data/retail_db/pyspark/departmentsObject")
#+END_SRC

* saveAsSequenceFile
#+BEGIN_SRC

dataRDD.map(lambda x: (None, x)).saveAsSequenceFile("/data/retail_db/pyspark/departmentsSeq")
dataRDD.map(lambda x: tuple(x.split(",", 1))).saveAsSequenceFile("/data/retail_db/pyspark/departmentsSeq")
dataRDD.map(lambda x: tuple(x.split(",", 1))).saveAsSequenceFile("/data/retail_db/pyspark/orders")
path="/data/retail_db/pyspark/departmentsSeq"

dataRDD.map(lambda x: tuple(x.split(",", 1))).saveAsNewAPIHadoopFile(path,"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat",keyClass="org.apache.hadoop.io.Text",valueClass="org.apache.hadoop.io.Text")
#+END_SRC
* reading sequence file
#+BEGIN_SRC
data = sc.sequenceFile("/data/retail_db/pyspark/departmentsSeq")
data = sc.sequenceFile("/data/retail_db/pyspark/orders")
data = sc.sequenceFile("/data/retail_db/pyspark/departmentsSeq", "org.apache.hadoop.io.IntWritable", "org.apache.hadoop.io.Text")
for rec in data.collect():
  print(rec)

from pyspark.sql import HiveContext
sqlContext = HiveContext(sc)
depts = sqlContext.sql("select * from departments")
for rec in depts.collect():
  print(rec)

sqlContext.sql("create table departmentsSpark as select * from departments")
depts = sqlContext.sql("select * from departmentsSpark")
for rec in depts.collect():
  print(rec)
#+END_SRC
* We can run hive INSERT, LOAD and any valid hive query in Hive context

- Make sure you copy departments.json to HDFS
- create departments.json on Linux file system
#+BEGIN_SRC

{"department_id":2, "department_name":"Fitness"}
{"department_id":3, "department_name":"Footwear"}
{"department_id":4, "department_name":"Apparel"}
{"department_id":5, "department_name":"Golf"}
{"department_id":6, "department_name":"Outdoors"}
{"department_id":7, "department_name":"Fan Shop"}
{"department_id":8, "department_name":"TESTING"}
{"department_id":8000, "department_name":"TESTING"}
#+END_SRC

* copying to HDFS (using linux command line)
#+BEGIN_SRC

hadoop fs -put departments.json /data/retail_db/pyspark

from pyspark import SQLContext
sqlContext = SQLContext(sc)
departmentsJson = sqlContext.jsonFile("/data/retail_db/pyspark/departments.json")
departmentsJson.registerTempTable("departmentsTable")
departmentsData = sqlContext.sql("select * from departmentsTable")
for rec in departmentsData.collect():
  print(rec)
#+END_SRC

* Writing data in json format
#+BEGIN_SRC

departmentsData.toJSON().saveAsTextFile("/data/retail_db/pyspark/departmentsJson")
#+END_SRC
* Validating the data

#+BEGIN_SRC

hadoop fs -cat /data/retail_db/pyspark/departmentsJson/part*
#+END_SRC

* <<<<<==============>>>>>
*  Developing word count program
-  Create a file and type few lines and save it as wordcount.txt and copy to HDFS
-  to /data/retail_db/wordcount.txt
#+BEGIN_SRC

data = sc.textFile("/data/retail_db/wordcount.txt")
dataFlatMap = data.flatMap(lambda x: x.split(" "))
dataMap = dataFlatMap.map(lambda x: (x, 1))
dataReduceByKey = dataMap.reduceByKey(lambda x,y: x + y)

dataReduceByKey.saveAsTextFile("/data/retail_db/wordcountoutput")

for i in dataReduceByKey.collect():
  print(i)
#+END_SRC
* <<<<<==============>>>>>

*  Join disparate datasets together using Spark
**  Problem statement, get the revenue and number of orders from order_items on daily basis
#+BEGIN_SRC

ordersRDD = sc.textFile("/data/retail_db/t0/orders")
orderItemsRDD = sc.textFile("/data/retail_db/t0/order_items")

ordersParsedRDD = ordersRDD.map(lambda rec: (int(rec.split(",")[0]), rec))
orderItemsParsedRDD = orderItemsRDD.map(lambda rec: (int(rec.split(",")[1]), rec))

ordersJoinOrderItems = orderItemsParsedRDD.join(ordersParsedRDD)
revenuePerOrderPerDay = ordersJoinOrderItems.map(lambda t: (t[1][1].split(",")[1], float(t[1][0].split(",")[4])))
#+END_SRC
**  Get order count per day
#+BEGIN_SRC

ordersPerDay = ordersJoinOrderItems.map(lambda rec: rec[1][1].split(",")[1] + "," + str(rec[0])).distinct()
ordersPerDayParsedRDD = ordersPerDay.map(lambda rec: (rec.split(",")[0], 1))
totalOrdersPerDay = ordersPerDayParsedRDD.reduceByKey(lambda x, y: x + y)
#+END_SRC
**  Get revenue per day from joined data
#+BEGIN_SRC

totalRevenuePerDay = revenuePerOrderPerDay.reduceByKey( \
lambda total1, total2: total1 + total2 \
)

for data in totalRevenuePerDay.collect():
  print(data)
#+END_SRC
**  Joining order count per day and revenue per day
#+BEGIN_SRC

finalJoinRDD = totalOrdersPerDay.join(totalRevenuePerDay)
for data in finalJoinRDD.take(5):
  print(data)
#+END_SRC
*  Using Hive
#+BEGIN_SRC

from pyspark.sql import HiveContext
sqlContext = HiveContext(sc)
sqlContext.sql("set spark.sql.shuffle.partitions=10");

joinAggData = sqlContext.sql("select o.order_date, round(sum(oi.order_item_subtotal), 2), \
count(distinct o.order_id) from orders o join order_items oi \
on o.order_id = oi.order_item_order_id \
group by o.order_date order by o.order_date")

for data in joinAggData.collect():
  print(data)
#+END_SRC
*  Using spark native sql
#+BEGIN_SRC

from pyspark.sql import SQLContext, Row
sqlContext = SQLContext(sc)
sqlContext.sql("set spark.sql.shuffle.partitions=10");

ordersRDD = sc.textFile("/data/retail_db/t0/orders")
ordersMap = ordersRDD.map(lambda o: o.split(","))
orders = ordersMap.map(lambda o: Row(order_id=int(o[0]), order_date=o[1], \
order_customer_id=int(o[2]), order_status=o[3]))
ordersSchema = sqlContext.inferSchema(orders)
ordersSchema.registerTempTable("orders")

orderItemsRDD = sc.textFile("/data/retail_db/t0/order_items")
orderItemsMap = orderItemsRDD.map(lambda oi: oi.split(","))
orderItems = orderItemsMap.map(lambda oi: Row(order_item_id=int(oi[0]), order_item_order_id=int(oi[1]), \
order_item_product_id=int(oi[2]), order_item_quantity=int(oi[3]), order_item_subtotal=float(oi[4]), \
order_item_product_price=float(oi[5])))
orderItemsSchema = sqlContext.inferSchema(orderItems)
orderItemsSchema.registerTempTable("order_items")

joinAggData = sqlContext.sql("select o.order_date, sum(oi.order_item_subtotal), \
count(distinct o.order_id) from orders o join order_items oi \
on o.order_id = oi.order_item_order_id \
group by o.order_date order by o.order_date")

for data in joinAggData.collect():
  print(data)
#+END_SRC
* <<<<<==============>>>>>

*  Calculate aggregate statistics (e.g., average or sum) using Spark
** sum
#+BEGIN_SRC

ordersRDD = sc.textFile("/data/retail_db/t0/orders")
ordersRDD.count()

orderItemsRDD = sc.textFile("/data/retail_db/t0/order_items")
orderItemsMap = orderItemsRDD.map(lambda rec: float(rec.split(",")[4]))
for i in orderItemsMap.take(5):
  print i

orderItemsReduce = orderItemsMap.reduce(lambda rev1, rev2: rev1 + rev2)
#+END_SRC
* Get max priced product from products table
** There is one record which is messing up default , delimiters
** Clean it up (we will see how we can filter with out deleting the record later)
hadoop fs -get /data/retail_db/t0/products
* Delete the record with product_id 685
hadoop fs -put -f products/part* /data/retail_db/t0/products

* pyspark script to get the max priced product
#+BEGIN_SRC

productsRDD = sc.textFile("/data/retail_db/t0/products")
productsMap = productsRDD.map(lambda rec: rec)
productsMap.reduce(lambda rec1, rec2: (rec1 if((rec1.split(",")[4] != "" and rec2.split(",")[4] != "") and float(rec1.split(",")[4]) >= float(rec2.split(",")[4])) else rec2))
#+END_SRC
* 
#+BEGIN_SRC
revenue = sc.textFile("/data/retail_db/t0/order_items").map(lambda rec: float(rec.split(",")[4])).reduce(lambda rev1, rev2: rev1 + rev2)
totalOrders = sc.textFile("/data/retail_db/t0/order_items").map(lambda rec: int(rec.split(",")[1])).distinct().count()
#+END_SRC
* Number of orders by status
#+BEGIN_SRC

ordersRDD = sc.textFile("/data/retail_db/t0/orders")
ordersMap = ordersRDD.map(lambda rec:  (rec.split(",")[3], 1))
for i in ordersMap.countByKey().items(): print(i)
#+END_SRC
* groupByKey is not very efficient
#+BEGIN_SRC

ordersByStatus = ordersMap.groupByKey().map(lambda t: (t[0], sum(t[1])))
ordersByStatus = ordersMap.reduceByKey(lambda acc, value: acc + value)
ordersMap = ordersRDD.map(lambda rec:  (rec.split(",")[3], rec))
ordersByStatus = ordersMap.aggregateByKey(0, lambda acc, value: acc+1, lambda acc, value: acc+value)
ordersByStatus = ordersMap.combineByKey(lambda value: 1, lambda acc, value: acc+1, lambda acc, value: acc+value)

for recs in ordersByStatus.collect():
  print(recs)
#+END_SRC
* Number of orders by order date and order status
* Key orderDate and orderStatus
#+BEGIN_SRC

ordersRDD = sc.textFile("/data/retail_db/t0/orders")
ordersMapRDD = ordersRDD.map(lambda rec: ((rec.split(",")[1], rec.split(",")[3]), 1))
ordersByStatusPerDay = ordersMapRDD.reduceByKey(lambda v1, v2: v1+v2)

for i in ordersByStatusPerDay.collect():
  print(i)
#+END_SRC

* Total Revenue per day
#+BEGIN_SRC

ordersRDD = sc.textFile("/data/retail_db/t0/orders")
orderItemsRDD = sc.textFile("/data/retail_db/t0/order_items")

ordersParsedRDD = ordersRDD.map(lambda rec: (rec.split(",")[0], rec))
orderItemsParsedRDD = orderItemsRDD.map(lambda rec: (rec.split(",")[1], rec))

ordersJoinOrderItems = orderItemsParsedRDD.join(ordersParsedRDD)
ordersJoinOrderItemsMap = ordersJoinOrderItems.map(lambda t: (t[1][1].split(",")[1], float(t[1][0].split(",")[4])))

revenuePerDay = ordersJoinOrderItemsMap.reduceByKey(lambda acc, value: acc + value)
for i in revenuePerDay.collect(): print(i)
#+END_SRC
* average
- average revenue per day
- Parse Orders (key order_id)
- Parse Order items (key order_item_order_id)
- Join the data sets
- Parse joined data and get (order_date, order_id) as key  and order_item_subtotal as value
- Use appropriate aggregate function to get sum(order_item_subtotal) for each order_date, order_id combination
- Parse data to discard order_id and get order_date as key and sum(order_item_subtotal) per order as value
- Use appropriate aggregate function to get sum(order_item_subtotal) per day and count(distinct order_id) per day
- Parse data and apply average logic
#+BEGIN_SRC

ordersRDD = sc.textFile("/data/retail_db/t0/orders")
orderItemsRDD = sc.textFile("/data/retail_db/t0/order_items")

ordersParsedRDD = ordersRDD.map(lambda rec: (rec.split(",")[0], rec))
orderItemsParsedRDD = orderItemsRDD.map(lambda rec: (rec.split(",")[1], rec))

ordersJoinOrderItems = orderItemsParsedRDD.join(ordersParsedRDD)
ordersJoinOrderItemsMap = ordersJoinOrderItems.map(lambda t: ((t[1][1].split(",")[1], t[0]), float(t[1][0].split(",")[4])))

revenuePerDayPerOrder = ordersJoinOrderItemsMap.reduceByKey(lambda acc, value: acc + value)
revenuePerDayPerOrderMap = revenuePerDayPerOrder.map(lambda rec: (rec[0][0], rec[1]))

revenuePerDay = revenuePerDayPerOrderMap.combineByKey( \
lambda x: (x, 1), \
lambda acc, revenue: (acc[0] + revenue, acc[1] + 1), \
lambda total1, total2: (round(total1[0] + total2[0], 2), total1[1] + total2[1]) \
)

revenuePerDay = revenuePerDayPerOrderMap.aggregateByKey( \
(0, 0), \
lambda acc, revenue: (acc[0] + revenue, acc[1] + 1), \
lambda total1, total2: (round(total1[0] + total2[0], 2), total1[1] + total2[1]) \
)

for data in revenuePerDay.collect():
  print(data)

avgRevenuePerDay = revenuePerDay.map(lambda x: (x[0], x[1][0]/x[1][1]))
#+END_SRC
* Customer id with max revenue
#+BEGIN_SRC

ordersRDD = sc.textFile("/data/retail_db/t0/orders")
orderItemsRDD = sc.textFile("/data/retail_db/t0/order_items")

ordersParsedRDD = ordersRDD.map(lambda rec: (rec.split(",")[0], rec))
orderItemsParsedRDD = orderItemsRDD.map(lambda rec: (rec.split(",")[1], rec))

ordersJoinOrderItems = orderItemsParsedRDD.join(ordersParsedRDD)
ordersPerDayPerCustomer = ordersJoinOrderItems.map(lambda rec: ((rec[1][1].split(",")[1], rec[1][1].split(",")[2]), float(rec[1][0].split(",")[4])))
revenuePerDayPerCustomer = ordersPerDayPerCustomer.reduceByKey(lambda x, y: x + y)

revenuePerDayPerCustomerMap = revenuePerDayPerCustomer.map(lambda rec: (rec[0][0], (rec[0][1], rec[1])))
topCustomerPerDaybyRevenue = revenuePerDayPerCustomerMap.reduceByKey(lambda x, y: (x if x[1] >= y[1] else y))
#+END_SRC
* Using regular function
#+BEGIN_SRC

def findMax(x, y):
  if(x[1] >= y[1]):
    return x
  else:
    return y

topCustomerPerDaybyRevenue = revenuePerDayPerCustomerMap.reduceByKey(lambda x, y: findMax(x, y))
#+END_SRC
*  Using Hive Context
spark-sql  or hive

for local file
#+BEGIN_SRC
create database retail_db;
create table retail_db.orders(order_id int, order_date string, order_customer_id int, order_status string) row format delimited fields terminated by ',' stored as textfile;
load data local inpath '/data/retail_db/t0/orders' into table orders;
#+END_SRC

** for hdfs

#+BEGIN_SRC
create database retail_db;
use retail_db;
create external table orders(order_id int, order_date string, order_customer_id int, order_status string) row format delimited fields terminated by ',' location '/data/retail_db/t0/orders';
create table order_items (order_item_id int, order_item_order_id int, order_item_product_id int, order_item_quantity int, order_item_subtotal float, order_item_product_price float) row format delimited fields terminated by ',' location '/data/retail_db/t0/order_items';
select * from retail_db.orders limit 5;
select * from retailo_db.order_items limit 5;
#+END_SRC

#+BEGIN_SRC

from pyspark.sql import HiveContext
hiveContext = HiveContext(sc)
hiveContext.sql("set spark.sql.shuffle.partitions=10");

data = hiveContext.sql(" \
select * from ( \
select o.order_date, o.order_customer_id, sum(oi.order_item_subtotal) order_item_subtotal \
from orders o join order_items oi \
on o.order_id = oi.order_item_order_id \
group by o.order_date, o.order_customer_id) q1 \
join \
(select q.order_date, max(q.order_item_subtotal) order_item_subtotal \
from (select o.order_date, o.order_customer_id, sum(oi.order_item_subtotal) order_item_subtotal \
from orders o join order_items oi \
on o.order_id = oi.order_item_order_id \
group by o.order_date, o.order_customer_id) q \
group by q.order_date) q2 \
on q1.order_date = q2.order_date and q1.order_item_subtotal = q2.order_item_subtotal \
order by q1.order_date")
#+END_SRC
*  This query works in hive
#+BEGIN_SRC

select * from (select q.order_date, q.order_customer_id, q.order_item_subtotal, 
max(q.order_item_subtotal) over (partition by q.order_date) max_order_item_subtotal 
from (select o.order_date, o.order_customer_id, sum(oi.order_item_subtotal) order_item_subtotal 
from orders o join order_items oi 
on o.order_id = oi.order_item_order_id 
group by o.order_date, o.order_customer_id) q) s
where s.order_item_subtotal = s.max_order_item_subtotal
order by s.order_date;

select * from (
select o.order_date, o.order_customer_id, sum(oi.order_item_subtotal) order_item_subtotal 
from orders o join order_items oi 
on o.order_id = oi.order_item_order_id 
group by o.order_date, o.order_customer_id) q1
join
(select q.order_date, max(q.order_item_subtotal) order_item_subtotal
from (select o.order_date, o.order_customer_id, sum(oi.order_item_subtotal) order_item_subtotal
from orders o join order_items oi
on o.order_id = oi.order_item_order_id
group by o.order_date, o.order_customer_id) q
group by q.order_date) q2
on q1.order_date = q2.order_date and q1.order_item_subtotal = q2.order_item_subtotal
order by q1.order_date;
#+END_SRC
* <<<<<==============>>>>>

*  Filter data into a smaller dataset using Spark
#+BEGIN_SRC

ordersRDD = sc.textFile("/data/retail_db/t0/orders")
for i in ordersRDD.filter(lambda line: line.split(",")[3] == "COMPLETE").take(5): print(i)

for i in ordersRDD.filter(lambda line: "PENDING" in line.split(",")[3]).take(5): print(i)

for i in ordersRDD.filter(lambda line: int(line.split(",")[0]) > 100).take(5): print(i)
 
for i in ordersRDD.filter(lambda line: int(line.split(",")[0]) > 100 or line.split(",")[3] in "PENDING").take(5): print(i)
 
for i in ordersRDD.filter(lambda line: int(line.split(",")[0]) > 1000 and ("PENDING" in line.split(",")[3] or line.split(",")[3] == ("CANCELLED"))).take(5): print(i)
 
for i in ordersRDD.filter(lambda line: int(line.split(",")[0]) > 1000 and line.split(",")[3] != ("COMPLETE")).take(5): print(i)
#+END_SRC
* Check if there are any cancelled orders with amount greater than 1000$
** Get only cancelled orders
** Join orders and order items
** Generate sum(order_item_subtotal) per order
** Filter data which amount to greater than 1000$
#+BEGIN_SRC

ordersRDD = sc.textFile("/data/retail_db/t0/orders")
orderItemsRDD = sc.textFile("/data/retail_db/t0/order_items")

ordersParsedRDD = ordersRDD.filter(lambda rec: rec.split(",")[3] in "CANCELED").map(lambda rec: (int(rec.split(",")[0]), rec))
orderItemsParsedRDD = orderItemsRDD.map(lambda rec: (int(rec.split(",")[1]), float(rec.split(",")[4])))
orderItemsAgg = orderItemsParsedRDD.reduceByKey(lambda acc, value: (acc + value))

ordersJoinOrderItems = orderItemsAgg.join(ordersParsedRDD)

for i in ordersJoinOrderItems.filter(lambda rec: rec[1][0] >= 1000).take(5): print(i)
#+END_SRC
* <<<<<==============>>>>>

*  Write a query that produces ranked or sorted data using Spark

** Global sorting and ranking
#+BEGIN_SRC

orders = sc.textFile("/data/retail_db/t0/orders")
for i in orders.map(lambda rec: (int(rec.split(",")[0]), rec)).sortByKey().collect(): print(i)
for i in orders.map(lambda rec: (int(rec.split(",")[0]), rec)).sortByKey(False).take(5): print(i)
for i in orders.map(lambda rec: (int(rec.split(",")[0]), rec)).top(5): print(i)
for i in orders.map(lambda rec: (int(rec.split(",")[0]), rec)).takeOrdered(5, lambda x: x[0]): print(i)
for i in orders.map(lambda rec: (int(rec.split(",")[0]), rec)).takeOrdered(5, lambda x: -x[0]): print(i)
for i in orders.takeOrdered(5, lambda x: int(x.split(",")[0])): print(i)
for i in orders.takeOrdered(5, lambda x: -int(x.split(",")[0])): print(i)

#+END_SRC
* By key sorting and ranking
#+BEGIN_SRC

def getAllSortByPrice(rec, bool):
  if(bool == False):
    x = sorted(rec[1], key = lambda k: -float(k.split(",")[4]))
  else:
    x = sorted(rec[1], key = lambda k: float(k.split(",")[4]))
  return (y for y in x)
    

def getAll(rec):
  return (x for x in rec[1])

def getFirstTwo(rec):
  x = [ ]
  ctr = 0
  for i in rec[1]:
    if(ctr < 2):
      x.append(i)
    ctr = ctr + 1
  return (y for y in x)

def getTop(rec):
  x = [ ]
  max = 0
  for i in rec[1]:
    prodPrice = float(i.split(",")[4])
    if(prodPrice > max):
      max = prodPrice
  for j in rec[1]:
    if(float(j.split(",")[4]) == max):
      x.append(j)
  return (y for y in x)

products = sc.textFile("/data/retail_db/t0/products")
productsMap = products.map(lambda rec: (rec.split(",")[1], rec))
productsGroupBy = productsMap.groupByKey()
for i in productsGroupBy.collect(): print(i)
#+END_SRC
* Get data sorted by product price per category
** You can use map or flatMap, if you want to see one record per line you need to use flatMap
** Map will return the list
#+BEGIN_SRC

for i in productsGroupBy.map(lambda rec: sorted(rec[1], key=lambda k: float(k.split(",")[4]))).take(100): print(i)
for i in productsGroupBy.map(lambda rec: sorted(rec[1], key=lambda k: float(k.split(",")[4]), reverse=True)).take(100): print(i)
#+END_SRC
* To get topN products by price in each category
#+BEGIN_SRC

def getTopN(rec, topN):
  x = [ ]
  x = list(sorted(rec[1], key=lambda k: float(k.split(",")[4]), reverse=True))
  import itertools
  return (y for y in list(itertools.islice(x, 0, topN)))

for i in productsMap.groupByKey().flatMap(lambda x: getTopN(x, 2)).collect(): print(i)
#+END_SRC
* To get topN priced products by category
#+BEGIN_SRC

def getTopDenseN(rec, topN):
  x = [ ]
  topNPrices = [ ]
  prodPrices = [ ]
  prodPricesDesc = [ ]
  for i in rec[1]:
    prodPrices.append(float(i.split(",")[4]))
  prodPricesDesc = list(sorted(set(prodPrices), reverse=True))
  import itertools
  topNPrices = list(itertools.islice(prodPricesDesc, 0, topN))
  for j in sorted(rec[1], key=lambda k: float(k.split(",")[4]), reverse=True):
    if(float(j.split(",")[4]) in topNPrices):
      x.append(j)
  return (y for y in x)

for i in productsMap.groupByKey().flatMap(lambda x: getTopDenseN(x, 2)).collect(): print(i)


productsFlatMap = products.flatMap(lambda rec: (rec.split(",")[1], float(rec.split(",")[4])))
for i in productsMap.groupByKey().flatMap(lambda x: getFirstTwo(x)).collect(): print(i)
for i in productsMap.groupByKey().flatMap(lambda x: getAllSortByPrice(x, True)).collect(): print(i)
for i in productsMap.groupByKey().flatMap(getAll).collect(): print(i)
for i in productsMap.groupByKey().flatMap(getTop).collect(): print(i)
#+END_SRC
* Sorting using queries
** Global sorting and ranking
#+BEGIN_SRC
select * from products order by product_price desc;
select * from products order by product_price desc limit 10;
#+END_SRC
* By key sorting
** Using order by is not efficient, it serializes
#+BEGIN_SRC

select * from products order by product_category_id, product_price desc;
#+END_SRC
* Using distribute by sort by (to distribute sorting and scale it up)
#+BEGIN_SRC

select * from products distribute by product_category_id sort by product_price desc;
#+END_SRC
* By key ranking (in Hive we can use windowing/analytic functions)
#+BEGIN_SRC

select * from (select p.*, 
dense_rank() over (partition by product_category_id order by product_price desc) dr
from products p
distribute by product_category_id) q
where dr <= 2 order by product_category_id, dr;
#+END_SRC


* schema for the test retail_db database

#+BEGIN_SRC

--
-- Table structure for table `categories`
--

DROP TABLE IF EXISTS `categories`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `categories` (
  `category_id` int(11) NOT NULL AUTO_INCREMENT,
  `category_department_id` int(11) NOT NULL,
  `category_name` varchar(45) NOT NULL,
  PRIMARY KEY (`category_id`)
) ENGINE=InnoDB AUTO_INCREMENT=59 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;


--
-- Table structure for table `customers`
--

DROP TABLE IF EXISTS `customers`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `customers` (
  `customer_id` int(11) NOT NULL AUTO_INCREMENT,
  `customer_fname` varchar(45) NOT NULL,
  `customer_lname` varchar(45) NOT NULL,
  `customer_email` varchar(45) NOT NULL,
  `customer_password` varchar(45) NOT NULL,
  `customer_street` varchar(255) NOT NULL,
  `customer_city` varchar(45) NOT NULL,
  `customer_state` varchar(45) NOT NULL,
  `customer_zipcode` varchar(45) NOT NULL,
  PRIMARY KEY (`customer_id`)
) ENGINE=InnoDB AUTO_INCREMENT=12436 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Table structure for table `departments`
--

DROP TABLE IF EXISTS `departments`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `departments` (
  `department_id` int(11) NOT NULL AUTO_INCREMENT,
  `department_name` varchar(45) NOT NULL,
  PRIMARY KEY (`department_id`)
) ENGINE=InnoDB AUTO_INCREMENT=8 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `departments`
--

LOCK TABLES `departments` WRITE;
/*!40000 ALTER TABLE `departments` DISABLE KEYS */;
INSERT INTO `departments` VALUES (2,'Fitness'),(3,'Footwear'),(4,'Apparel'),(5,'Golf'),(6,'Outdoors'),(7,'Fan Shop');
/*!40000 ALTER TABLE `departments` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `order_items`
--

DROP TABLE IF EXISTS `order_items`;
CREATE TABLE `order_items` (
  `order_item_id` int(11) NOT NULL AUTO_INCREMENT,
  `order_item_order_id` int(11) NOT NULL,
  `order_item_product_id` int(11) NOT NULL,
  `order_item_quantity` tinyint(4) NOT NULL,
  `order_item_subtotal` float NOT NULL,
  `order_item_product_price` float NOT NULL,
  PRIMARY KEY (`order_item_id`)
) ENGINE=InnoDB AUTO_INCREMENT=172199 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Table structure for table `orders`
--

DROP TABLE IF EXISTS `orders`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `orders` (
  `order_id` int(11) NOT NULL AUTO_INCREMENT,
  `order_date` datetime NOT NULL,
  `order_customer_id` int(11) NOT NULL,
  `order_status` varchar(45) NOT NULL,
  PRIMARY KEY (`order_id`)
) ENGINE=InnoDB AUTO_INCREMENT=68884 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Table structure for table `products`
--

DROP TABLE IF EXISTS `products`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `products` (
  `product_id` int(11) NOT NULL AUTO_INCREMENT,
  `product_category_id` int(11) NOT NULL,
  `product_name` varchar(45) NOT NULL,
  `product_description` varchar(255) NOT NULL,
  `product_price` float NOT NULL,
  `product_image` varchar(255) NOT NULL,
  PRIMARY KEY (`product_id`)
) ENGINE=InnoDB AUTO_INCREMENT=1346 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

#+END_SRC

*** postgresql 
#+BEGIN_SRC

CREATE TABLE departments (
  department_id INT NOT NULL,
  department_name VARCHAR(45) NOT NULL,
  PRIMARY KEY (department_id)
);

--
-- Table structure for table categories
--

CREATE TABLE categories (
  category_id INT NOT NULL,
  category_department_id INT NOT NULL,
  category_name VARCHAR(45) NOT NULL,
  PRIMARY KEY (category_id)
); 

--
-- Table structure for table products
--

CREATE TABLE products (
  product_id INT NOT NULL,
  product_category_id INT NOT NULL,
  product_name VARCHAR(45) NOT NULL,
  product_description VARCHAR(255) NOT NULL,
  product_price FLOAT NOT NULL,
  product_image VARCHAR(255) NOT NULL,
  PRIMARY KEY (product_id)
);

--
-- Table structure for table customers
--

CREATE TABLE customers (
  customer_id INT NOT NULL,
  customer_fname VARCHAR(45) NOT NULL,
  customer_lname VARCHAR(45) NOT NULL,
  customer_email VARCHAR(45) NOT NULL,
  customer_password VARCHAR(45) NOT NULL,
  customer_street VARCHAR(255) NOT NULL,
  customer_city VARCHAR(45) NOT NULL,
  customer_state VARCHAR(45) NOT NULL,
  customer_zipcode VARCHAR(45) NOT NULL,
  PRIMARY KEY (customer_id)
); 

--
-- Table structure for table orders
--

CREATE TABLE orders (
  order_id INT NOT NULL,
  order_date TIMESTAMP NOT NULL,
  order_customer_id INT NOT NULL,
  order_status VARCHAR(45) NOT NULL,
  PRIMARY KEY (order_id)
);

--
-- Table structure for table order_items
--

CREATE TABLE order_items (
  order_item_id INT NOT NULL,
  order_item_order_id INT NOT NULL,
  order_item_product_id INT NOT NULL,
  order_item_quantity INT NOT NULL,
  order_item_subtotal FLOAT NOT NULL,
  order_item_product_price FLOAT NOT NULL,
  PRIMARY KEY (order_item_id)
);

#+END_SRC
